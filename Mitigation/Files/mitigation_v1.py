import os
import json
import re
import traceback
import numpy as np
from langchain_ollama import OllamaLLM
import signal

# CoT Template as defined in the thesis
COT_TEMPLATE = """Description:
You are an AI assistant and an expert in Python programming and code evaluation. You will be given a case that needs evaluation. The case consists of three main components:

1. **Prompt:** A textual instruction describing the expected behavior of a Python function. This includes requirements and a test case showcasing the function header with example inputs and expected outputs.
2. **Generated Code:** The Python code generated by an LLM based on the given prompt. The code always contains some errors (hallucinations) that need to be identified.
3. **Test Results:** The output from running test cases against the generated code.

Task:
Your objective is to analyze the provided input and identify errors within the generated code. The code may contain multiple hallucinations, including logical errors, syntax issues, or inconsistencies with the provided prompt. Your analysis should focus on understanding the mistakes and providing a clear explanation of how to correct them.

After analyzing the errors, you should **repair the code** and present a corrected version that adheres to the given requirements.

Output Format:
Your response should contain two sections:
1. **Analysis:** A detailed explanation of the errors found in the generated code and how they should be addressed.
2. **Repaired Code:** The corrected Python code.

Formatting Rules:
- The **analysis** should be provided first.
- The **repaired code** must be formatted as a Python markdown block, like this:
  [PYTHON] CODE [/PYTHON]

Prompt:
{prompt_case}

Generated Code:
{generated_code}

Test cases:
{test_cases}
    """

# Model (can be changed)
MODEL_NAME = "llama2:13b"

# Model parameters
llm = OllamaLLM(
        model = MODEL_NAME,
        temperature = 0.8,
        top_k = 50,
        top_p = 0.95
    )

# Source file
CASE_FILE = "failed_results.jsonl"

# Defining the path, where we want to store our results.
# I have decided to create a subdirectory for each model
model_dir = os.path.join("/home/marc/Documents/BA/MBPP/Mitigation", MODEL_NAME)
os.makedirs(model_dir, exist_ok=True)

# Setting the n parameter for the unbaised pass@k
NUM_SAMPLES = 10

# Unbaised pass@k formula taken from https://arxiv.org/pdf/2107.03374
def pass_at_k(n, c, k):
    if n - c < k:
        return 1.0
    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))

# Used for my timeout logic
def signal_handler(signal, frame):
    raise TimeoutError("Timeout occured!")

# Mitigation function
def run_mitigation():

    with open(CASE_FILE, "r", encoding="utf-8") as dataset:
        cases = [json.loads(line) for line in dataset if line.strip()]

    pass_at_1_list = []
    pass_at_3_list = []
    pass_at_5_list = []
    total_success = 0
    NUM_PROMPTS = len(cases)

    for index, case in enumerate(cases, start=1):

        prompt_text = case["prompt"]
        generated_code = case["generated_code"]
        test_list = case.get("test_results", [])
        assertion_raw = case.get("assertion", "")
        assertion_tests = [test.strip() for test in assertion_raw.split("\n") if test.strip()]

        sample_results = []
        correct_count = 0
        
        for sample in range(1, NUM_SAMPLES + 1):
            sample_log = []
            sample_log.append(f"Sample {sample}:")
            
            cot_prompt = COT_TEMPLATE.format(
                prompt_case=prompt_text,
                generated_code=generated_code,
                test_cases=test_list
            )

            try:
                response = llm.invoke(cot_prompt)
            except Exception as e:
                sample_log.append("There is a problem when calling the model!")
                sample_results.append( (False, "\n".join(sample_log)) )
                continue

            sample_log.append("LLM Analysis:")
            sample_log.append(response)
            
            code = re.search(r'\[PYTHON\](.*?)\[/PYTHON\]', response, re.DOTALL)
            if not code:
                code = re.search(r"```python(.*?)```", response, re.DOTALL)
                if not code:
                    code = re.search(r"```(.*?)```", response, re.DOTALL)
                    if not code:
                        code = ""
            code = code.group(1)

            sample_log.append("Repaired Code:")
            sample_log.append(code)
            
            test_results = []
            namespace = {}
            sample_success = True
            
            signal.signal(signal.SIGALRM, signal_handler)
            signal.alarm(5)

            try:
                exec(code, namespace)
                sample_log.append("Code execution: SUCCESS")
            except Exception as e:
                error_type = e.__class__.__name__
                error_details = str(e).splitlines()
                if error_details:
                    error_msg = error_details[0]
                else:
                    error_msg = "No detailed description"
                test_results.append(f"Execution error: {error_type}: {error_msg}")
                sample_log.append(f"Execution error: {error_type}: {error_msg}")
                sample_success = False
            
            signal.alarm(0)

            if sample_success:
                for test in assertion_tests:
                    signal.signal(signal.SIGALRM, signal_handler)
                    signal.alarm(5)
                    try:
                        result = run_test(test, namespace)
                        test_results.append(result)
                    except Exception as e:
                        error_type = e.__class__.__name__
                        test_results.append(f"FAILED: {test} => {error_type}")
                        sample_log.append(f"FAILED: {test} => {error_type}")
                        sample_success = False
                    finally:
                        signal.alarm(0)
            
            if all("Passed" in result for result in test_results):
                    total_success += 1
            else:
                sample_success = False


            sample_log.append("Test Results:")
            sample_log.extend(test_results)

            if sample_success:
                correct_count += 1
                sample_log.append("Sample result: PASSED")
            else:
                sample_log.append("Sample result: FAILED")
            
            sample_results.append( (sample_success, "\n".join(sample_log)) )
            print(f"Processed prompt {index} sample {sample}.")


        pass_at_1 = pass_at_k(NUM_SAMPLES, correct_count, 1)
        pass_at_3 = pass_at_k(NUM_SAMPLES, correct_count, 3)
        pass_at_5 = pass_at_k(NUM_SAMPLES, correct_count, 5)
        
        pass_at_1_list.append(pass_at_1)
        pass_at_3_list.append(pass_at_3)
        pass_at_5_list.append(pass_at_5)
        
        prompt_log = []
        prompt_log.append(f"Prompt {index}:")
        prompt_log.append(f"Prompt text: {prompt_text}\n")
        prompt_log.append("Results of 10 samples:")

        for i, (success, sample_text) in enumerate(sample_results, start=1):
            status = "PASSED" if success else "FAILED"
            prompt_log.append(f"--- Sample {i}: {status} ---")
            prompt_log.append(sample_text)
            prompt_log.append("\n")

        prompt_log.append("Pass@k results for this prompt:")
        prompt_log.append(f"   pass@1: {pass_at_1:.4f}")
        prompt_log.append(f"   pass@3: {pass_at_3:.4f}")
        prompt_log.append(f"   pass@5: {pass_at_5:.4f}")
        
        prompt_log_file = os.path.join(model_dir, f"prompt_{index}.txt")
        with open(prompt_log_file, "w", encoding="utf-8") as log_file:
            log_file.write("\n".join(prompt_log))
        print(f"Prompt {index} log saved to {prompt_log_file}")


    # Average of the problem-wise pass@k
    avg_pass_at_1 = np.mean(pass_at_1_list)
    avg_pass_at_3 = np.mean(pass_at_3_list)
    avg_pass_at_5 = np.mean(pass_at_5_list)

    summary = []
    summary.append("=== Pass@k results across all prompts ===")
    summary.append(f"Amount of prompts: {NUM_PROMPTS * NUM_SAMPLES}")
    summary.append(f"Total successful samples: {total_success}")
    summary.append(f"Average pass@1: {avg_pass_at_1:.4f}")
    summary.append(f"Average pass@3: {avg_pass_at_3:.4f}")
    summary.append(f"Average pass@5: {avg_pass_at_5:.4f}")
    summary_file = os.path.join(model_dir, "mitigation_summary.txt")

    with open(summary_file, "w", encoding="utf-8") as sum_file:
        sum_file.write("\n".join(summary))

    print(f"Summary saved to {summary_file}")


# Used to evaluate both sides of the function and also get the exact values.
# The regular assert function does not tell you something about the expected and actual value
# only about whether the test case passed or failed.
def run_test(test, global_context):
    try:
        match = re.match(r"assert\s+(.+?)\s*==\s*(.+)", test)
        if match:
            function_call = match.group(1).strip()
            expected_result = eval(match.group(2).strip(), global_context, global_context)
            actual_result = eval(function_call, global_context, global_context)
            return f"{test} => Passed" if actual_result == expected_result else f"{test} => Failed (Expected: {expected_result}, Got: {actual_result})"
        else:
            exec(test, global_context, global_context)
            return f"{test} => Passed"
    except Exception as e:
        return f"{test} => Error: {e}"


if __name__ == "__main__":
    run_mitigation()

