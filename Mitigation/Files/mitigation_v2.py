import os
import json
import re
import numpy as np
import signal
from langchain_ollama import OllamaLLM
import openai

# === CONFIG ===
MODEL_NAME = "deepseek-r1:70b"
CASE_FILE = "failed_results.jsonl"
NUM_SAMPLES = 10
TIMEOUT_SECONDS = 5
API_KEY = "Nu-Uh"

# === WORKING DIR ===
model_dir = os.path.join("/home/marc/Documents/BA/MBPP/Mitigation", MODEL_NAME)
os.makedirs(model_dir, exist_ok=True)

# === OLLAMA SETTINGS ===
llm = OllamaLLM(
    model=MODEL_NAME,
    temperature=0.8,
    top_k=50,
    top_p=0.95
)

# === COT TEMPLATE ===
COT_TEMPLATE = """Description:
You are an AI assistant and an expert in Python programming and code evaluation. You will be given a case that needs evaluation. The case consists of three main components:

1. **Prompt:** A textual instruction describing the expected behavior of a Python function. This includes requirements and a test case showcasing the function header with example inputs and expected outputs.
2. **Generated Code:** The Python code generated by an LLM based on the given prompt. The code always contains some errors (hallucinations) that need to be identified.
3. **Test Results:** The output from running test cases against the generated code.

Task:
Your objective is to analyze the provided input and identify errors within the generated code. The code may contain multiple hallucinations, including logical errors, syntax issues, or inconsistencies with the provided prompt. Your analysis should focus on understanding the mistakes and providing a clear explanation of how to correct them.

After analyzing the errors, you should **repair the code** and present a corrected version that adheres to the given requirements.

Output Format:
Your response should contain two sections:
1. **Analysis:** A detailed explanation of the errors found in the generated code and how they should be addressed.
2. **Repaired Code:** The corrected Python code.

Formatting Rules:
- The **analysis** should be provided first.
- The **repaired code** must be formatted as a Python markdown block, like this:
  [PYTHON] CODE [/PYTHON]

Prompt:
{prompt_case}

Generated Code:
{generated_code}

Test cases:
{test_cases}
"""


def signal_handler(signum, frame):
    """
    Handling exections, mainly used for timeout during the test execution.
    """
    raise TimeoutError("Timeout occurred!")


def load_cases(case_file):
    """
    Load the json containing all error logs.
    """
    cases = []
    with open(case_file, "r", encoding="utf-8") as f:
        for line in f:
            cases.append(json.loads(line))
    return cases


def extract_code_from_response(response):
    """
    Extract the code from a given LLM answer.
    """
    match = re.search(r'\[PYTHON\](.*?)\[/PYTHON\]', response, re.DOTALL)
    if not match:
        match = re.search(r"```python(.*?)```", response, re.DOTALL)
    if not match:
        match = re.search(r"```(.*?)```", response, re.DOTALL)

    if match:
        return match.group(1)
    return ""


def safe_exec(code, namespace, timeout = TIMEOUT_SECONDS):
    """
    Execute the code, using the timeout logic and timeout handler defined above.
    Used for getting the error message: https://stackoverflow.com/a/14529489
    """
    signal.signal(signal.SIGALRM, signal_handler)
    signal.alarm(timeout)
    try:
        exec(code, namespace)
        return True, "Code execution: SUCCESS"
    except Exception as e:
        error_type = e.__class__.__name__
        error_details = str(e).splitlines()
        if error_details:
            error_msg = error_details[0]
        else:
            error_msg = "No detailed description"
        return False, f"Execution error: {error_type}: {error_msg}"
    finally:
        signal.alarm(0)


def run_test(test, global_context):
    """
    Run the test cases, evaluate each side of an assertion seperately.
    Allows for more detailed feedback in the test logs.
    """
    try:
        match = re.match(r"assert\s+(.+?)\s*==\s*(.+)", test)
        if match:
            function_call = match.group(1).strip()
            expected_result = eval(match.group(2).strip(), global_context, global_context)
            actual_result = eval(function_call, global_context, global_context)
            if actual_result == expected_result:
                return f"{test} => Passed"
            return f"{test} => Failed (Expected: {expected_result}, Got: {actual_result})"
        else:
            exec(test, global_context, global_context)
            return f"{test} => Passed"
    except Exception as e:
        return f"{test} => Error: {e}"


def process_sample(prompt_text, generated_code, test_list, assertion_tests):
    """
    Builts the prompt for the LLM, logs and returns the results.
    """
    log_messages = []

    cot_prompt = COT_TEMPLATE.format(
        prompt_case=prompt_text, 
        generated_code=generated_code, 
        test_cases=test_list
    )

    try:
        """
        We need this part to switch between Ollama and OpenAI models.
        """
        
        response = llm.invoke(cot_prompt)
        
        """
        response = openai.ChatCompletion.create(
            model = "o3-mini",
            messages = [{"role": "user", "content": cot_prompt}],
            api_key = API_KEY
        )
        response = response["choices"][0]["message"]["content"]
        """
        
    except Exception:
        log_messages.append("There is a problem when calling the model!")
        return False, "\n".join(log_messages)

    log_messages.append("LLM Analysis:")
    log_messages.append(response)

    
    code = extract_code_from_response(response)
    log_messages.append("Repaired Code:")
    log_messages.append(code if code else "No code extracted.")

    
    namespace = {}
    sample_success = True
    exec_success, exec_msg = safe_exec(code, namespace)
    log_messages.append(exec_msg)

    test_results = []
    if not exec_success:
        sample_success = False
    else:
        for test_cmd in assertion_tests:
            signal.signal(signal.SIGALRM, signal_handler)
            signal.alarm(TIMEOUT_SECONDS)
            try:
                result = run_test(test_cmd, namespace)
                test_results.append(result)
                if "Failed" in result or "Error" in result:
                    sample_success = False
            except Exception as e:
                error_type = e.__class__.__name__
                fail_msg = f"FAILED: {test_cmd} => {error_type}"
                test_results.append(fail_msg)
                log_messages.append(fail_msg)
                sample_success = False
            finally:
                signal.alarm(0)

    log_messages.append("Test Results:")
    log_messages.extend(test_results)

    if sample_success:
        log_messages.append("Sample result: PASSED")
    else:
        log_messages.append("Sample result: FAILED")

    return sample_success, "\n".join(log_messages)

def pass_at_k(n, c, k):
    """
    Unbaised pass@k formula taken from https://arxiv.org/pdf/2107.03374.
    n = number of samples for each prompt, c = correct samples, k = pass@k parameter 
    """
    if n - c < k:
        return 1.0
    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))


def evaluate_single_prompt(index, case, n_samples):
    """
    This function calculates the pass@k values for a given sample.
    """
    prompt_text = case["prompt"]
    generated_code = case["generated_code"]
    test_list = case.get("test_results", [])
    assertion_raw = case.get("assertion", "")
    assertion_tests = [test.strip() for test in assertion_raw.split("\n") if test.strip()]

    sample_results = []
    correct_count = 0

    for sample_i in range(1, n_samples + 1):
        success, log_text = process_sample(
            prompt_text,
            generated_code,
            test_list,
            assertion_tests
        )
        sample_results.append((success, log_text))
        if success:
            correct_count += 1
        print(f"Processed prompt {index} sample {sample_i}.")

    pass_at_1 = pass_at_k(n_samples, correct_count, 1)
    pass_at_3 = pass_at_k(n_samples, correct_count, 3)
    pass_at_5 = pass_at_k(n_samples, correct_count, 5)

    return pass_at_1, pass_at_3, pass_at_5, sample_results


def main():
    """
    Main benchmark function.
    """
    cases = load_cases(CASE_FILE)

    pass_at_1_list = []
    pass_at_3_list = []
    pass_at_5_list = []
    total_success = 0
    num_prompts = len(cases)

    for idx, case in enumerate(cases, start=1):
        p_at_1, p_at_3, p_at_5, sample_results = evaluate_single_prompt(idx, case, NUM_SAMPLES)
        pass_at_1_list.append(p_at_1)
        pass_at_3_list.append(p_at_3)
        pass_at_5_list.append(p_at_5)

        success_count = sum(1 for s, _ in sample_results if s)
        total_success += success_count

        prompt_text = case["prompt"]
        prompt_log = []
        prompt_log.append(f"Prompt {idx}:")
        prompt_log.append(f"Prompt text: {prompt_text}\n")
        prompt_log.append(f"Results of {NUM_SAMPLES} samples:")

        for i, (success, sample_text) in enumerate(sample_results, start=1):
            status = "PASSED" if success else "FAILED"
            prompt_log.append(f"--- Sample {i}: {status} ---")
            prompt_log.append(sample_text)
            prompt_log.append("\n")

        prompt_log.append("Pass@k results for this prompt:")
        prompt_log.append(f"   pass@1: {p_at_1:.4f}")
        prompt_log.append(f"   pass@3: {p_at_3:.4f}")
        prompt_log.append(f"   pass@5: {p_at_5:.4f}")

        prompt_log_file = os.path.join(model_dir, f"prompt_{idx}.txt")
        with open(prompt_log_file, "w", encoding="utf-8") as log_file:
            log_file.write("\n".join(prompt_log))

        print(f"Prompt {idx} log saved to {prompt_log_file}")

    avg_pass_at_1 = np.mean(pass_at_1_list)
    avg_pass_at_3 = np.mean(pass_at_3_list)
    avg_pass_at_5 = np.mean(pass_at_5_list)

    summary = []
    summary.append("=== Pass@k results across all prompts ===")
    summary.append(f"Number of prompts: {num_prompts}")
    summary.append(f"Samples per prompt: {NUM_SAMPLES}")
    summary.append(f"Total samples: {num_prompts * NUM_SAMPLES}")
    summary.append(f"Total successful samples: {total_success}")
    summary.append(f"Average pass@1: {avg_pass_at_1:.4f}")
    summary.append(f"Average pass@3: {avg_pass_at_3:.4f}")
    summary.append(f"Average pass@5: {avg_pass_at_5:.4f}")

    summary_file = os.path.join(model_dir, "mitigation_summary.txt")
    with open(summary_file, "w", encoding="utf-8") as sum_file:
        sum_file.write("\n".join(summary))

    print(f"Summary saved to {summary_file}")


if __name__ == "__main__":
    main()